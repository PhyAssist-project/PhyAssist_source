{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_ENDPOINT=https://hf-mirror.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading PEFT model\n",
    "PEFT_MODEL = \"idegroup/PhyAssist\"\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def format_prompt(message, history):\n",
    "    prompt = \"<s>\"\n",
    "    # prompt += f\"[INST] As a psychology counselor assistant, provide an assessment and plan for the following counseling notes. Please present a summary, don't make it so long. Present in lines.: {message} [/INST]\"\n",
    "    # for user_prompt, bot_response in history:\n",
    "    #     prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "    #     prompt += f\" {bot_response}</s> \"\n",
    "    prompt += f\"[INST]As a psychology counselor assistant, you need to provide an assessment and plan for the following counseling post and question. \\nYour answer shoule be formed by to parts: the problem assessment and problem solution plan. Don't make your generation so long.  Please present the plan in lines. \\nPost:{message} Question: Does the patient has mental health problem? [/INST]\"\n",
    "    return prompt\n",
    "def generate(\n",
    "    prompt, history, temperature=0.9, max_new_tokens=1024, top_p=0.95, repetition_penalty=1.0,\n",
    "):\n",
    "    temperature = float(temperature)\n",
    "    if temperature < 1e-2:\n",
    "        temperature = 1e-2\n",
    "    top_p = float(top_p)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    formatted_prompt = format_prompt(f\"{prompt}\", history)\n",
    "    # print(formatted_prompt)\n",
    "    # stream = client(formatted_prompt, **generate_kwargs, stream=True, details=True, return_full_text=False)\n",
    "    inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to('cuda:0')\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=320)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        yield generated_text\n",
    "    thread.join()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "examples=[\n",
    "    [\"Patient is feeling stressed due to work and has trouble sleeping.\", None, None, None, None, None],\n",
    "    [\"Client is dealing with relationship issues and is seeking advice on communication strategies.\", None, None, None, None, None],\n",
    "    [\"Individual has recently experienced a loss and is having difficulty coping with grief.\", None, None, None, None, None],\n",
    "]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=generate,\n",
    "    chatbot=gr.Chatbot(show_label=False, show_share_button=False, show_copy_button=True, likeable=True, layout=\"panel\"),\n",
    "    title=\"Psychological Assistant: Expert in Assessment and Strategic Planning\",\n",
    "    description=\"Enter counseling notes to generate an assessment and plan.\",\n",
    "    examples=examples,\n",
    "    concurrency_limit=20,\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
