{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_ENDPOINT=https://hf-mirror.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0062105655670166016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 19,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8c1a45faa45a7884559a93d24e9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading PEFT model\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL_LIST[0])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_MODEL_LIST = [\"adapter_nbertagnolli/checkpoint-660\",\n",
    "                   \"adapter_mentalLLama_Final/checkpoint-960\",\n",
    "                   \"adapter_mentalLLama_dreaddit/checkpoint-1120\",\n",
    "                   \"adapter_mentalLLama_DR/checkpoint-320\"]\n",
    "model = peft_base_model\n",
    "for i in range(len(PEFT_MODEL_LIST)):\n",
    "    model = PeftModel.from_pretrained(model, PEFT_MODEL_LIST[i])\n",
    "    model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "# Function to generate responses from both original model and PEFT model and compare their answers.\n",
    "def generate_answer(post, question):\n",
    "  system_prompt = f\"<s>[INST]You will get a Post and a Question And you have to answer the question based on the post, If you think the post is meaningless, just say \\\"No\\\". The anwser should following this format: Yes/No, Reasoning: (Your Reasoning)\\n\"\n",
    "  post_prompt = f\"<s>[INST]Consider this post: \\\"{post}\\\" \"\n",
    "  question_prompt = f\"Question: {question}[/INST]\"\n",
    "\n",
    "  final_prompt = post_prompt + question_prompt\n",
    "\n",
    "  device = \"cuda:0\"\n",
    "  dashline = \"-\".join(\"\" for i in range(50))\n",
    "\n",
    "  encoding = tokenizer(final_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "  outputs = model.generate(input_ids=encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = tokenizer.eos_token_id, \\\n",
    "                                                                                                                do_sample = True, eos_token_id = tokenizer.eos_token_id, attention_mask = encoding.attention_mask, \\\n",
    "                                                                                                                   temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n",
    "  text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  print(dashline)\n",
    "  print(f'MODEL RESPONSE:\\n{text_output}')\n",
    "  print(dashline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to avoid a relapse? I\\'ve been having a particularly rough year; I attempted suicide, the love of my life left me, I failed my year at university, I\\'ve been physically assaulted, I\\'ve been sexually assaulted and today my grandfather died. I feel like I\\'m running on empty and doing the bare minimum to survive for myself while trying to be a rock to others. My emotional resilience has been wiped out. I feel \\\"okay\\\" but I\\'ve felt like this in the past and it has turned out that I\\'ve just been lying to myself and making things worse. Any advice on how to manage/process my emotions? Or just how to better understand myself? \"\n",
    "question = \"Does the poster suffers from depression?\"\n",
    "generate_answer(query, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to avoid a relapse? I\\'ve been having a particularly rough year; I attempted suicide, the love of my life left me, I failed my year at university, I\\'ve been physically assaulted, I\\'ve been sexually assaulted and today my grandfather died. I feel like I\\'m running on empty and doing the bare minimum to survive for myself while trying to be a rock to others. My emotional resilience has been wiped out. I feel \\\"okay\\\" but I\\'ve felt like this in the past and it has turned out that I\\'ve just been lying to myself and making things worse. Any advice on how to manage/process my emotions? Or just how to better understand myself? \"\n",
    "question = \"Does the poster suffers from depression?\"\n",
    "generate_answer(query, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Ancestry.com - is it safe? Hello!: ) I'm a new user so if this post ends up in a weird place/thread, pls bear w me. Has anyone used Ancestry.com lately? They offer a two week trial but still require your credit card information. Is this suspicious? It doesn't cost anything for the first two weeks, and then you can end the trial so you don't have to pay anything even then, but I just feel a little uncomfortable dialing my credit card information on there online.... Am I being too doubtfull? Share your experiences? Thank you! If my English isn't perfect, that's because I'm not a native speaker.\"\n",
    "question = \"Does the poster suffers from depression?\"\n",
    "generate_answer(query, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Dini ticaret haline getirenler\" \n",
    "question = \"Does the poster suffers from depression?\"\n",
    "generate_answer(query, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation Data provide by MentalLLAMA is in MentalLLaMA_dataset/test_data/test_complete/SAD.csv\n",
    "# Generate the response data using MentalLLaMA_dataset/src/IMHI.py\n",
    "!python MentalLLaMA_dataset/src/IMHI.py --model_path \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\\\n",
    "    --adapter_path mistral7b_mentalLLama_Final/checkpoint-960\\\n",
    "    --batch_size 8 \\\n",
    "    --model_output_path SAD \\\n",
    "    --test_dataset IMHI-completion \\\n",
    "    --test_data_path MentalLLaMA_dataset/test_data/test_complete/SAD.csv \\\n",
    "    --cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model trained by MentalLLaMA to test the accuracy of the generated response\n",
    "!python MentalLLaMA_dataset/src/label_inference.py --model_path Tianlin668 \\\n",
    "                               --data_path MentalLLaMA_dataset/model_output/Irf \\\n",
    "                               --data_output_path MentalLLaMA_dataset/model_output/result/ \\\n",
    "                               --cuda --calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartScore: Explanation Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the bart_score to calculate the quality of the generated response\n",
    "!python MentalLLaMA_dataset/src/score.py --gen_dir_name SAD \\\n",
    "    --score_method bart_score \\\n",
    "    --cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
